# -*- coding: utf-8 -*-
"""Sales Forecasting.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N12ZkutgcE8tHJyDHFsp_FEE-pgXOI3C

# Task 7: Walmart Sales Forecasting

## 1. Importing Libraries

> This section imports the required Python libraries for data analysis, preprocessing, visualization, and modeling.

- NumPy, Pandas → data handling

- Matplotlib, Seaborn → visualization

- Scikit-learn → preprocessing, model training, evaluation

- XGBoost → advanced regression model
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

"""## 2. Loading the Dataset

> The Walmart Sales Forecast dataset from Kaggle is loaded using kagglehub.

- features.csv → additional features (holidays, fuel price, etc.)

- train.csv → weekly sales data

- stores.csv → store information
"""

# Install dependencies as needed:
# pip install kagglehub[pandas-datasets]
import kagglehub
from kagglehub import KaggleDatasetAdapter

# Set the path to the file you'd like to load
file_path = "features.csv"
file_path2 = "train.csv"
file_path3 = "stores.csv"


# Load the latest version
features_df = kagglehub.load_dataset(
  KaggleDatasetAdapter.PANDAS,
  "aslanahmedov/walmart-sales-forecast",
  file_path,
)

train_df = kagglehub.load_dataset(
  KaggleDatasetAdapter.PANDAS,
  "aslanahmedov/walmart-sales-forecast",
  file_path2,
)

stores_df = kagglehub.load_dataset(
  KaggleDatasetAdapter.PANDAS,
  "aslanahmedov/walmart-sales-forecast",
  file_path3,
)

"""## 3. Data Cleaning and Preprocessing

- Removed unused Markdown columns.

- Dropped records with zero or negative sales.

- Extracted year, month, day from date.

- Converted categorical variables (IsHoliday, Type) into numerical format using Label Encoding.
"""

df = train_df.merge(features_df, how='left').merge(stores_df, how='left')
print(df.shape)
df.head()

df.isnull().sum()

df.drop(['MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5'],inplace=True,axis=1)

df.isnull().sum()

df[df['Weekly_Sales']<=0]

df= df.drop(df[df['Weekly_Sales']<=0].index)

df.shape

df["year"] = pd.DatetimeIndex(df["Date"]).year
df["month"] = pd.DatetimeIndex(df["Date"]).month
df["day"] = pd.DatetimeIndex(df["Date"]).day
df.drop(['Date'],inplace=True,axis=1)
df.head()

le = LabelEncoder()

df['IsHoliday'] = le.fit_transform(df['IsHoliday'])
df['Type'] = le.fit_transform(df['Type'])
df

"""## 4. Feature and Target Selection

- Features (X): all columns except Weekly_Sales

- Target (y): Weekly_Sales
"""

X = df.drop(['Weekly_Sales'],axis=1)
y = df['Weekly_Sales']
#

"""## 5. Baseline Model – Linear Regression

> A simple Linear Regression model was trained to establish baseline performance.

### 5.1 Data Splitting

- Training Set: 80%

- Testing Set: 20%
"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""### 5.2 Model Training and Prediction

> Fitted linear regression model.

> Predicted test set sales.
"""

from sklearn.linear_model import LinearRegression
rf = LinearRegression()
rf.fit(X_train, y_train)

y_pred = rf.predict(X_test)

"""### 5.3 Model Evaluation

> Metrics: MSE, MAE, R² Score

> Visualization: Actual vs Predicted Weekly Sales
"""

print("mean square error = ", mean_squared_error(y_test, y_pred))
print("mean absolute error = ", mean_absolute_error(y_test, y_pred))
print("r2 score = ", r2_score(y_test, y_pred))

# Make sure indices align
y_test = y_test.reset_index(drop=True)
y_pred = pd.Series(y_pred)

plt.figure(figsize=(14,6))
plt.plot(y_test, label="Actual Sales", color="blue", alpha=0.7)
plt.plot(y_pred, label="Predicted Sales", color="red", alpha=0.7)
plt.title("Actual vs Predicted Weekly Sales")
plt.xlabel("Time (Test Samples)")
plt.ylabel("Weekly Sales")
plt.legend()
plt.show()

"""## 6. Advanced Model – XGBoost Regressor

> An XGBoost model was applied to improve prediction accuracy.


"""

# aplying XGBRegressor
train = df[df['year']< 2012]
test = df[df['year'] >= 2012]

"""### 6.1 Time-Aware Split

> Training Data: years before 2012

> Testing Data: year 2012 and beyond
"""

X_train = train.drop(['Weekly_Sales'],axis=1)
y_train = train['Weekly_Sales']
X_test = test.drop(['Weekly_Sales'],axis=1)
y_test = test['Weekly_Sales']
#

"""### 6.2 Model Training and Prediction

> Tuned hyperparameters (n_estimators=500, max_depth=8, etc.).

> Trained on training set, predicted on test set.
"""

from xgboost import XGBRegressor

model = XGBRegressor(
    n_estimators=500,
    learning_rate=0.05,
    max_depth=8,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

"""### 6.3 Model Evaluation and Feature Importance

> Metrics: MSE, MAE, R² Score

- Plotted Feature Importance

- Compared actual vs predicted weekly sales.
"""

print("mean square error = ", mean_squared_error(y_test, y_pred))
print("mean absolute error = ", mean_absolute_error(y_test, y_pred))
print("r2 score = ", r2_score(y_test, y_pred))

import xgboost as xgb
xgb.plot_importance(model, max_num_features=10, importance_type='gain')
plt.show()

# Make sure indices align
y_test = y_test.reset_index(drop=True)
y_pred = pd.Series(y_pred)

plt.figure(figsize=(14,6))
plt.plot(y_test, label="Actual Sales", color="blue", alpha=0.7)
plt.plot(y_pred, label="Predicted Sales", color="red", alpha=0.7)
plt.title("Actual vs Predicted Weekly Sales")
plt.xlabel("Time (Test Samples)")
plt.ylabel("Weekly Sales")
plt.legend()
plt.show()

"""## 7. Results and Comparison
### 7.1 Linear Regression Performance

> Mean Square Error: 464,612,749

>Mean Absolute Error: 14,523

> R² Score: 0.09 (poor fit)

### 7.2 XGBoost Performance

> Mean Square Error: 22,122,995

> Mean Absolute Error: 2,703

> R² Score: 0.95 (excellent fit)

## 8. Conclusion

> The baseline Linear Regression model was unable to capture complex trends in the data.

> The XGBoost model provided a significant improvement, achieving a high R² score (~95%).

> Additional improvements can be made by including lag features, rolling averages, and seasonal decomposition for capturing time-series patterns.
"""





